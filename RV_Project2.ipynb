{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mredelis/CAP4453-Robot-Vision-Spr23/blob/main/RV_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f9L6g7VJiDh"
      },
      "source": [
        "#Project 2#\n",
        "\n",
        "##CAP 4453: Robot Vision##\n",
        "\n",
        "Programming language for the assignment is Python and you will use PyTorch framework for deep learning.\n",
        "\n",
        "Deliver the project as a colab note.\n",
        "\n",
        "The colab note must include your code (properly running), and a short write-up about the results and your observations from each task. For each task, you should report the training/testing accuracy for the best model. Analyze the variation in training/testing loss as you train your network and discuss what you observe. Also, discuss the time required for training your network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di3Wx7pwJ_fv"
      },
      "source": [
        "## Part 1. Neural Networks ##\n",
        "\n",
        "You goal in this assignment is to train neural networks for digit classiﬁcation. You will use MNIST dataset which has around 70K images of handwritten digits. You will be provided the template code for this assignment, and you must make some changes to the network and analyze the results after these changes.\n",
        "\n",
        "Your tasks:\n",
        "  1. [15%] Simple neural network: In this task, your goal is to design a neural network with 3 layers (input, hidden,and output) and in each layer you should use less than 20 neurons. There should be NO activation functions in your network. There are 10 classes in MNIST dataset corresponding to each digit, so this will be a 10-way classiﬁcation network.\n",
        "\n",
        "  2. [15%] Activation function: In this task, you will add activation function to your network. Use ReLU activation in all your layers from the previous task.\n",
        "  \n",
        "  3. [20%] Deep network: In this task, your goal is to increase the size of the network. Your new network should have more than 4 layers and each layer should have more than 200 neurons. Use ReLU activation in all the layers. Note that the last layer will still have only 10 neurons as this is a 10-way classiﬁcation network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and normalize MNIST ###"
      ],
      "metadata": {
        "id": "GH76jnK4nzkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision # dataloader for common datasets including CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Relevant for variables for Part 1. These are the same for tasks 1, 2, and 3\n",
        "# batch_size_part1 = 10\n",
        "# learning_rate_part1 = 0.1\n",
        "# num_epochs_part1 = 20\n",
        "# batch_size_part1 = 64\n",
        "# learning_rate_part1 = 1e-3\n",
        "# num_epochs_part1 = 20\n",
        "# batch_size_part1 = 10\n",
        "# learning_rate_part1 = 1e-3\n",
        "# num_epochs_part1 = 20\n",
        "batch_size_part1 = 100\n",
        "learning_rate_part1 = 0.1\n",
        "num_epochs_part1 = 10\n",
        "\n",
        "# Create transformations to apply to each data sample \n",
        "# Can specify variations such as image flip, color flip, random crop, ...\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "dataset1 = torchvision.datasets.MNIST('./data/', \n",
        "                                      train=True, \n",
        "                                      download=True,\n",
        "                                      transform=transform)\n",
        "\n",
        "dataset2 = torchvision.datasets.MNIST('./data/', \n",
        "                                      train=False,\n",
        "                                      transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, \n",
        "                                           batch_size=batch_size_part1, \n",
        "                                           shuffle=True, \n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, \n",
        "                                          batch_size=batch_size_part1, \n",
        "                                          shuffle=False, \n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "BtfwsFVG80Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Convolutional Neural Network for Tasks 1, 2, 3 ###"
      ],
      "metadata": {
        "id": "MYYjpRIDoZOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Define various layers\n",
        "        self.fc1 = nn.Linear(in_features=28*28, out_features=100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "\n",
        "        # More than 4 layers and each layer should have more than 200 neurons\n",
        "        self.fc_layer1 = nn.Linear(in_features=28*28, out_features=700)\n",
        "        self.fc_layer2 = nn.Linear(700, 500)\n",
        "        self.fc_layer3 = nn.Linear(500, 400)\n",
        "        self.fc_layer4 = nn.Linear(400, 10)\n",
        "        \n",
        "        # This will select the forward pass function based on mode for the ConvNet.\n",
        "        # During creation of each ConvNet model, you will assign one of the valid mode.\n",
        "        # This will fix the forward function (and the network graph) for the entire training/testing\n",
        "        if mode == 1:\n",
        "            self.forward = self.model_1\n",
        "        elif mode == 2:\n",
        "            self.forward = self.model_2\n",
        "        elif mode == 3:\n",
        "            self.forward = self.model_3\n",
        "        else: \n",
        "            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n",
        "            exit(0)\n",
        "             \n",
        "    # task 1\n",
        "    def model_1(self, X):\n",
        "        # Three fully connected layers without activation\n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        X = self.fc1(X)  \n",
        "        X = self.fc2(X)  \n",
        "        X = self.fc3(X)    \n",
        "                        \n",
        "        return X\n",
        "        \n",
        "    # task 2\n",
        "    def model_2(self, X):\n",
        "        # Train with activation (use model 1 from task 1)\n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = F.relu(self.fc3(X))\n",
        "        \n",
        "        return X\n",
        "\n",
        "    # task 3\n",
        "    def model_3(self, X):\n",
        "        # Change number of fully connected layers and number of neurons from model 2 in task 2       \n",
        "        X = torch.flatten(X, start_dim=1)\n",
        "        X = F.relu(self.fc_layer1(X))\n",
        "        X = F.relu(self.fc_layer2(X))\n",
        "        X = F.relu(self.fc_layer3(X))\n",
        "        X = F.relu(self.fc_layer4(X))\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "t1uKqF2-ocmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test functions ##"
      ],
      "metadata": {
        "id": "ZuHdG8wHNzvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwJ2BjQ8zd4b"
      },
      "outputs": [],
      "source": [
        "def train_part1(model, train_loader, optimizer, criterion,epoch, batch_size):\n",
        "    '''\n",
        "    Trains the model for an epoch and optimizes it.\n",
        "    model: The model to train. \n",
        "    train_loader: dataloader for training samples.\n",
        "    optimizer: optimizer to use for model parameter updates.\n",
        "    criterion: used to compute loss for prediction and target \n",
        "    epoch: Current epoch to train for.\n",
        "    batch_size: Batch size to be used.\n",
        "    '''\n",
        "    \n",
        "    # Set model to train mode before each epoch\n",
        "    model.train()\n",
        "    \n",
        "    # Empty list to store losses \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Iterate over entire training samples (1 epoch)\n",
        "    for batch_idx, batch_sample in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        data, target = batch_sample\n",
        "\n",
        "        # zero the parameter gradients     \n",
        "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Do forward pass for current set of data\n",
        "        output = model(data) \n",
        "\n",
        "        # Compute loss based on criterion\n",
        "        loss = criterion(output, target)\n",
        "        # Computes gradient based on final loss\n",
        "        loss.backward()     \n",
        "        # Store loss\n",
        "        losses.append(loss.item())\n",
        "        # print(f\"Losses in train: {losses}\\n\")\n",
        "        \n",
        "        # Optimize model parameters based on learning rate and gradient \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get predicted index by selecting maximum log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        \n",
        "        # Count correct predictions overall \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "    train_loss = float(np.mean(losses))\n",
        "    train_acc = correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
        "        100. * correct / ((batch_idx+1) * batch_size)))\n",
        "    \n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzLHbW573MtT"
      },
      "outputs": [],
      "source": [
        "def test_part1(model, test_loader):\n",
        "    '''\n",
        "    Tests the model.\n",
        "    model: The model to train. \n",
        "    test_loader: dataloader for test samples.\n",
        "    '''\n",
        "    \n",
        "    # Set model to eval mode to notify all layers.\n",
        "    model.eval()   \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, sample in enumerate(test_loader):\n",
        "          data, target = sample      \n",
        "\n",
        "          # Predict for data by doing forward pass\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute loss based on same criterion as training\n",
        "          loss = F.cross_entropy(output, target, reduction='mean')\n",
        "          \n",
        "          # Append loss to overall test loss\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "          # Count correct predictions overall \n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "          \n",
        "\n",
        "    test_loss = float(np.mean(losses))\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate network, define a loss function and optimizer ###"
      ],
      "metadata": {
        "id": "z3wZmgPBDSGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "import datetime \n",
        "from datetime import timedelta\n",
        "\n",
        "for i in range(1, 4): # Models 1, 2, 3\n",
        "\n",
        "  print(f\"Task {i}: Model {i}\\n\")\n",
        "  t = time.process_time() # to keep track of time\n",
        "\n",
        "  # Instantiate network\n",
        "  model = ConvNet(i)\n",
        "\n",
        "  # Initialize the criterion for loss computation \n",
        "  criterion = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate_part1, weight_decay=1e-7)\n",
        "\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in range(1, num_epochs_part1+1):\n",
        "    print(\"\\nEpoch: \", epoch)\n",
        "    train_loss, train_accuracy = train_part1(model, train_loader, optimizer, criterion , epoch, batch_size_part1)\n",
        "    test_loss, test_accuracy = test_part1(model, test_loader)\n",
        "    \n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "\n",
        "  print()     \n",
        "  print(\"Accuracy is {:2.2f}\".format(best_accuracy))\n",
        "\n",
        "  print(f\"Training and evaluation finished for Task {i}: Model {i}\\n\")\n",
        "\n",
        "  elapsed_time = time.process_time() - t\n",
        "  print(f\"Elapsed time: {str(datetime.timedelta(seconds=elapsed_time))}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gseggXNZDTDR",
        "outputId": "329d9014-6710-47e0-b42e-74d75bf39c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1: Model 1\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 0.3950, Accuracy: 53218/60000 (89%)\n",
            "Test set: Average loss: 0.3077, Accuracy: 9117/10000 (91%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 0.3128, Accuracy: 54644/60000 (91%)\n",
            "Test set: Average loss: 0.2991, Accuracy: 9159/10000 (92%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 0.3032, Accuracy: 54823/60000 (91%)\n",
            "Test set: Average loss: 0.2955, Accuracy: 9166/10000 (92%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 0.2959, Accuracy: 54990/60000 (92%)\n",
            "Test set: Average loss: 0.2882, Accuracy: 9183/10000 (92%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 0.2902, Accuracy: 55023/60000 (92%)\n",
            "Test set: Average loss: 0.2872, Accuracy: 9181/10000 (92%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.2860, Accuracy: 55160/60000 (92%)\n",
            "Test set: Average loss: 0.3008, Accuracy: 9138/10000 (91%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.2845, Accuracy: 55153/60000 (92%)\n",
            "Test set: Average loss: 0.2945, Accuracy: 9210/10000 (92%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.2819, Accuracy: 55236/60000 (92%)\n",
            "Test set: Average loss: 0.2916, Accuracy: 9186/10000 (92%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.2792, Accuracy: 55220/60000 (92%)\n",
            "Test set: Average loss: 0.2988, Accuracy: 9144/10000 (91%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.2790, Accuracy: 55269/60000 (92%)\n",
            "Test set: Average loss: 0.2857, Accuracy: 9225/10000 (92%)\n",
            "\n",
            "Accuracy is 92.25\n",
            "Training and evaluation finished for Task 1: Model 1\n",
            "\n",
            "Elapsed time: 0:00:38.177109\n",
            "\n",
            "Task 2: Model 2\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 0.4516, Accuracy: 51844/60000 (86%)\n",
            "Test set: Average loss: 0.1959, Accuracy: 9369/10000 (94%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 0.1435, Accuracy: 57460/60000 (96%)\n",
            "Test set: Average loss: 0.1240, Accuracy: 9605/10000 (96%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 0.1000, Accuracy: 58173/60000 (97%)\n",
            "Test set: Average loss: 0.1002, Accuracy: 9680/10000 (97%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 0.0753, Accuracy: 58620/60000 (98%)\n",
            "Test set: Average loss: 0.0856, Accuracy: 9720/10000 (97%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 0.0599, Accuracy: 58889/60000 (98%)\n",
            "Test set: Average loss: 0.0784, Accuracy: 9750/10000 (98%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.0490, Accuracy: 59072/60000 (98%)\n",
            "Test set: Average loss: 0.0859, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.0401, Accuracy: 59272/60000 (99%)\n",
            "Test set: Average loss: 0.0752, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.0323, Accuracy: 59432/60000 (99%)\n",
            "Test set: Average loss: 0.0720, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.0266, Accuracy: 59534/60000 (99%)\n",
            "Test set: Average loss: 0.0733, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.0220, Accuracy: 59619/60000 (99%)\n",
            "Test set: Average loss: 0.0786, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Accuracy is 97.85\n",
            "Training and evaluation finished for Task 2: Model 2\n",
            "\n",
            "Elapsed time: 0:00:39.882114\n",
            "\n",
            "Task 3: Model 3\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 0.4167, Accuracy: 52791/60000 (88%)\n",
            "Test set: Average loss: 0.1417, Accuracy: 9560/10000 (96%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 0.1169, Accuracy: 57892/60000 (96%)\n",
            "Test set: Average loss: 0.1198, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 0.0737, Accuracy: 58655/60000 (98%)\n",
            "Test set: Average loss: 0.0774, Accuracy: 9754/10000 (98%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 0.0505, Accuracy: 59093/60000 (98%)\n",
            "Test set: Average loss: 0.0750, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 0.0357, Accuracy: 59329/60000 (99%)\n",
            "Test set: Average loss: 0.0606, Accuracy: 9814/10000 (98%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.0265, Accuracy: 59509/60000 (99%)\n",
            "Test set: Average loss: 0.0634, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.0189, Accuracy: 59666/60000 (99%)\n",
            "Test set: Average loss: 0.0627, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.0122, Accuracy: 59805/60000 (100%)\n",
            "Test set: Average loss: 0.0677, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.0081, Accuracy: 59896/60000 (100%)\n",
            "Test set: Average loss: 0.0632, Accuracy: 9827/10000 (98%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.0055, Accuracy: 59929/60000 (100%)\n",
            "Test set: Average loss: 0.0693, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "Accuracy is 98.27\n",
            "Training and evaluation finished for Task 3: Model 3\n",
            "\n",
            "Elapsed time: 0:02:31.789335\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss results ##\n",
        "\n",
        "|                        | Task 1      |Task 2    |Task 3     |\n",
        "| ---------------------  | ----------- |----------|---------- |\n",
        "| Training Accuracy      | 92%         |99%       |100%       |\n",
        "| Test Accuracy          | 92%         |98%       |98%        |\n",
        "| Training time in Colab | 37 sec      |38 sec    |~2 1/2 min |\n",
        "\n",
        "<br> \n",
        "\n",
        "The neural network for Task 1 does not have an activation function and every neuron will only perform a linear transformation using the weights and biases. Even though, the accuracy yield is high because the MNIST dataset is a very easy-to-learn dataset with few complex patterns from the data. \n",
        "\n",
        "The accuracy is improved in Task 2 when the ReLu activation function is added to all the layers from Task 1. The activation function introduces the non-linearity to the network helping the model to learn important information while suppressing the irrelevant data points by normalizing the output of any neuron between 1 and 0.  \n",
        "\n",
        "Accuracy is slightly improved in Task 3 given the network has more neurons. But the main difference from Task 2 is the increase in the time it takes to train the network.   "
      ],
      "metadata": {
        "id": "giY0m8WdEdgs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StFPmXgPKNsz"
      },
      "source": [
        "## Part 2. Convolutional Neural Network ##\n",
        "\n",
        "Your goal in this assignment is to train convolutional neural networks for image classiﬁcation. You will use CIFAR-10 dataset, which has 60K color images (each has size 32x32 pixels) from 10 classes. You will be provided the template code for this assignment, and you have to make some changes to the network and analyze the results after these changes. For each of these tasks, use learning rate of 0.1 and batch size of 100 and train them for 10 epochs each.\n",
        " `\n",
        "Your tasks:\n",
        "\n",
        "  4. [20%] Simple CNN: In this task, your goal is to design a convolutional neural network with 2 convolutional layers (Conv2d) layers and 2 pooling layers, followed by 2 fully connected layers. Both Conv2d layers should have 10 ﬁlters (output channels). The second Conv2d layer’s input channels should match ﬁrst Conv2d layer’s output channels. Use a kernel size of 3 for all convolutional layers. Apply ReLU activation to each Conv2d. Each Conv2d layer should be followed by max_pool2d layer with kernel size of 2. The output features from convolution after ﬂattening will be 360, so set the input features in fully connected layer accordingly (You can use fc1_model1) for this. There are 10 classes in CIFAR-10 dataset, so this will be a 10-way\n",
        "classiﬁcation network. You can use model_0 from the template and modify that to ﬁt this task.\n",
        "\n",
        "  5. [15%] Increase ﬁlters: In this task, you will increase the ﬁlters in each Conv2d layer in your network. Learn 20 kernels for the ﬁrst Conv2d layer(set output channels to 20). For the second Conv2d layer learn 40 kernels (set the output channels to 40). Match input channels for second Conv2d layer with output channels of ﬁrst Conv2d layer. Since this will change output feature size after second Conv2d layer, use fc1_model2 with 1440 input features for this task.\n",
        "\n",
        "  6. [15%] Large CNN: In this task, your goal is to increase the size of the network. Take the network from previous task and add one more Conv2d layer with 40 ﬁlters (set both input and output channels to 40). Do not add a max pooling layer after this third convolution layer. Use fc1_model3 with 640 input features for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIWv6IW-sA1"
      },
      "source": [
        "### Load and normalize CIFAR-10 ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFNaPsBBoIwm",
        "outputId": "6ed547ee-a559-4ae7-e924-1422772283a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision # dataloader for common datasets including CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Relevant for variables for Part 2. These are the same for tasks 4, 5, and 6\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10\n",
        "\n",
        "num_classes = 10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Output of torchvision datasets are PILImage images of range [0, 1]. \n",
        "# Transform them to Tensors of normalized range [-1, 1].\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Define datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                        train=True,\n",
        "                                        download=True, \n",
        "                                        transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True, \n",
        "                                          num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', \n",
        "                                       train=False,\n",
        "                                       download=True, \n",
        "                                       transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, \n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2)\n",
        "\n",
        "# functions to show an image\n",
        "# def imshow(img):\n",
        "#   img = img / 2 + 0.5     # unnormalize\n",
        "#   npimg = img.numpy()\n",
        "#   plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#   plt.show()\n",
        "\n",
        "# get some random training images\n",
        "# dataiter = iter(trainloader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# for i in range(2):  # show just the frogs\n",
        "#   imshow(torchvision.utils.make_grid(images[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnEpEvzKE1A"
      },
      "source": [
        "### Define Convolutional Neural Network for Tasks 4, 5, 6 ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ywPNtdB-k8Y"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=40, out_channels=40, kernel_size=3)\n",
        "\n",
        "        self.fc1_model1 = nn.Linear(360, 100)  # This is first fully connected layer for step 1.\n",
        "        self.fc1_model2 = nn.Linear(1440, 100) # This is first fully connected layer for step 2.\n",
        "        self.fc1_model3 = nn.Linear(640, 100)  # This is first fully connected layer for step 3\n",
        "\n",
        "        self.fc2 = nn.Linear(100, 10)       # This is 2nd fully connected layer for all models.\n",
        "        \n",
        "        # This will select the forward pass function based on mode for the ConvNet.\n",
        "        # Based on the question, there are 3 modes available for tasks 4, 5 and 6.\n",
        "        # During creation of each ConvNet model, you will assign one of the valid mode.\n",
        "        # This will fix the forward function (and the network graph) for the entire training/testing\n",
        "        if mode == 1:\n",
        "            self.forward = self.model_1\n",
        "        elif mode == 2:\n",
        "            self.forward = self.model_2\n",
        "        elif mode == 3:\n",
        "            self.forward = self.model_3\n",
        "        else: \n",
        "            print(\"Invalid mode \", mode, \"selected. Select between 1-3\")\n",
        "            exit(0)\n",
        "\n",
        "    # Model for Task 4 Simple CNN\n",
        "    def model_1(self, x): \n",
        "      x = self.conv_layer1(x) \n",
        "      x = F.relu(x)  \n",
        "      x = F.max_pool2d(x, kernel_size=2) \n",
        "      x = self.conv_layer2(x)  \n",
        "      x = F.relu(x)  \n",
        "      x = F.max_pool2d(x, kernel_size=2)   \n",
        "      x = torch.flatten(x, start_dim=1) # flatten all dimensions except batch \n",
        "      x = F.relu(self.fc1_model1(x))\n",
        "      x = self.fc2(x)\n",
        "        \n",
        "      return x\n",
        "\n",
        "    # Model for Task 5 Increase filters\n",
        "    def model_2(self, x): \n",
        "      x = F.max_pool2d(F.relu(self.conv1(x) ), kernel_size=2) \n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2)   \n",
        "      x = torch.flatten(x, start_dim=1) # flatten all dimensions except batch \n",
        "      x = F.relu(self.fc1_model2(x))\n",
        "      x = self.fc2(x)\n",
        "        \n",
        "      return x\n",
        "\n",
        "    # Model for Task 6 Increase filters\n",
        "    def model_3(self, x): \n",
        "      x = F.max_pool2d(F.relu(self.conv1(x) ), kernel_size=2) \n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2) \n",
        "      x = F.relu(self.conv3(x))   \n",
        "      x = torch.flatten(x, start_dim=1) # flatten all dimensions except batch \n",
        "      x = F.relu(self.fc1_model3(x))\n",
        "      x = self.fc2(x)\n",
        "        \n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and test functions ##"
      ],
      "metadata": {
        "id": "YOCfQdPRM-ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, epoch, batch_size):\n",
        "    '''\n",
        "    Trains the model for an epoch and optimizes it.\n",
        "    model: The model to train. \n",
        "    train_loader: dataloader for training samples.\n",
        "    optimizer: optimizer to use for model parameter updates.\n",
        "    criterion: used to compute loss for prediction and target \n",
        "    epoch: Current epoch to train for.\n",
        "    batch_size: Batch size to be used.\n",
        "    '''\n",
        "    \n",
        "    # Set model to train mode before each epoch\n",
        "    model.train()\n",
        "    \n",
        "    # Empty list to store losses \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Iterate over entire training samples (1 epoch)\n",
        "    for batch_idx, batch_sample in enumerate(train_loader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        data, target = batch_sample\n",
        "\n",
        "        # zero the parameter gradients     \n",
        "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Do forward pass for current set of data\n",
        "        output = model(data)     \n",
        "        # Compute loss based on criterion\n",
        "        loss = criterion(output, target)       \n",
        "        # Computes gradient based on final loss\n",
        "        loss.backward()\n",
        "        \n",
        "        # Store loss\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Optimize model parameters based on learning rate and gradient \n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get predicted index by selecting maximum log-probability\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        \n",
        "        # Count correct predictions overall \n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()     \n",
        "\n",
        "    train_loss = float(np.mean(losses))\n",
        "    train_acc = correct / ((batch_idx+1) * batch_size)\n",
        "\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
        "        100. * correct / ((batch_idx+1) * batch_size)))\n",
        "    \n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "dYgAnblXM9re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    '''\n",
        "    Tests the model.\n",
        "    model: The model to train. \n",
        "    test_loader: dataloader for test samples.\n",
        "    '''\n",
        "    \n",
        "    # Set model to eval mode to notify all layers.\n",
        "    model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct = 0\n",
        "    \n",
        "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, sample in enumerate(test_loader):\n",
        "          data, target = sample      \n",
        "\n",
        "          # Predict for data by doing forward pass\n",
        "          output = model(data)\n",
        "          \n",
        "          # Compute loss based on same criterion as training\n",
        "          loss = F.cross_entropy(output, target, reduction='mean')\n",
        "          \n",
        "          # Append loss to overall test loss\n",
        "          losses.append(loss.item())\n",
        "          \n",
        "          # Get predicted index by selecting maximum log-probability\n",
        "          pred = output.argmax(dim=1, keepdim=True)\n",
        "          \n",
        "          # Count correct predictions overall \n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss = float(np.mean(losses))\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "metadata": {
        "id": "UkS8cd5NNMz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q56_IPX9tnff"
      },
      "source": [
        "### Instantiate network, define a loss function and optimizer ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv7hG92GxodW",
        "outputId": "9d290a19-c5b0-4e7b-e36e-fe21a2b3ca94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 4: Model 1\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 1.8305, Accuracy: 16899/50000 (34%)\n",
            "Test set: Average loss: 1.6324, Accuracy: 4003/10000 (40%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.4796, Accuracy: 23277/50000 (47%)\n",
            "Test set: Average loss: 1.3479, Accuracy: 5147/10000 (51%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.3274, Accuracy: 26231/50000 (52%)\n",
            "Test set: Average loss: 1.2766, Accuracy: 5430/10000 (54%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.2284, Accuracy: 28236/50000 (56%)\n",
            "Test set: Average loss: 1.2018, Accuracy: 5748/10000 (57%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 1.1472, Accuracy: 29783/50000 (60%)\n",
            "Test set: Average loss: 1.1816, Accuracy: 5859/10000 (59%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 1.0817, Accuracy: 31086/50000 (62%)\n",
            "Test set: Average loss: 1.1924, Accuracy: 5825/10000 (58%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 1.0275, Accuracy: 32045/50000 (64%)\n",
            "Test set: Average loss: 1.1006, Accuracy: 6136/10000 (61%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.9850, Accuracy: 32770/50000 (66%)\n",
            "Test set: Average loss: 1.0774, Accuracy: 6230/10000 (62%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.9449, Accuracy: 33511/50000 (67%)\n",
            "Test set: Average loss: 1.0674, Accuracy: 6301/10000 (63%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.9148, Accuracy: 33954/50000 (68%)\n",
            "Test set: Average loss: 1.0988, Accuracy: 6240/10000 (62%)\n",
            "\n",
            "Accuracy is 63.01\n",
            "Training and evaluation finished for Task 4: Model 1\n",
            "\n",
            "Elapsed time: 0:03:25.544715\n",
            "\n",
            "Task 5: Model 2\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 1.7559, Accuracy: 18215/50000 (36%)\n",
            "Test set: Average loss: 1.4853, Accuracy: 4719/10000 (47%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.3293, Accuracy: 26231/50000 (52%)\n",
            "Test set: Average loss: 1.2839, Accuracy: 5293/10000 (53%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.1320, Accuracy: 30046/50000 (60%)\n",
            "Test set: Average loss: 1.0907, Accuracy: 6135/10000 (61%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.0070, Accuracy: 32304/50000 (65%)\n",
            "Test set: Average loss: 1.0190, Accuracy: 6408/10000 (64%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 0.9150, Accuracy: 33964/50000 (68%)\n",
            "Test set: Average loss: 0.9515, Accuracy: 6692/10000 (67%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.8308, Accuracy: 35492/50000 (71%)\n",
            "Test set: Average loss: 0.9674, Accuracy: 6705/10000 (67%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.7580, Accuracy: 36839/50000 (74%)\n",
            "Test set: Average loss: 0.9433, Accuracy: 6733/10000 (67%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.6922, Accuracy: 37963/50000 (76%)\n",
            "Test set: Average loss: 0.8908, Accuracy: 6998/10000 (70%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.6282, Accuracy: 39017/50000 (78%)\n",
            "Test set: Average loss: 0.9476, Accuracy: 6901/10000 (69%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.5695, Accuracy: 40034/50000 (80%)\n",
            "Test set: Average loss: 0.9633, Accuracy: 6933/10000 (69%)\n",
            "\n",
            "Accuracy is 69.98\n",
            "Training and evaluation finished for Task 5: Model 2\n",
            "\n",
            "Elapsed time: 0:06:44.527581\n",
            "\n",
            "Task 6: Model 3\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Train set: Average loss: 1.9768, Accuracy: 13654/50000 (27%)\n",
            "Test set: Average loss: 1.6290, Accuracy: 4076/10000 (41%)\n",
            "\n",
            "Epoch:  2\n",
            "Train set: Average loss: 1.5415, Accuracy: 21868/50000 (44%)\n",
            "Test set: Average loss: 1.6337, Accuracy: 4056/10000 (41%)\n",
            "\n",
            "Epoch:  3\n",
            "Train set: Average loss: 1.3326, Accuracy: 26084/50000 (52%)\n",
            "Test set: Average loss: 1.2250, Accuracy: 5660/10000 (57%)\n",
            "\n",
            "Epoch:  4\n",
            "Train set: Average loss: 1.1904, Accuracy: 28870/50000 (58%)\n",
            "Test set: Average loss: 1.1589, Accuracy: 5895/10000 (59%)\n",
            "\n",
            "Epoch:  5\n",
            "Train set: Average loss: 1.0750, Accuracy: 31068/50000 (62%)\n",
            "Test set: Average loss: 1.1609, Accuracy: 5997/10000 (60%)\n",
            "\n",
            "Epoch:  6\n",
            "Train set: Average loss: 0.9923, Accuracy: 32577/50000 (65%)\n",
            "Test set: Average loss: 1.0468, Accuracy: 6392/10000 (64%)\n",
            "\n",
            "Epoch:  7\n",
            "Train set: Average loss: 0.9178, Accuracy: 34007/50000 (68%)\n",
            "Test set: Average loss: 0.9899, Accuracy: 6591/10000 (66%)\n",
            "\n",
            "Epoch:  8\n",
            "Train set: Average loss: 0.8519, Accuracy: 35079/50000 (70%)\n",
            "Test set: Average loss: 0.9279, Accuracy: 6819/10000 (68%)\n",
            "\n",
            "Epoch:  9\n",
            "Train set: Average loss: 0.7978, Accuracy: 36003/50000 (72%)\n",
            "Test set: Average loss: 0.9049, Accuracy: 6923/10000 (69%)\n",
            "\n",
            "Epoch:  10\n",
            "Train set: Average loss: 0.7438, Accuracy: 36876/50000 (74%)\n",
            "Test set: Average loss: 0.8947, Accuracy: 6979/10000 (70%)\n",
            "\n",
            "Accuracy is 69.79\n",
            "Training and evaluation finished for Task 6: Model 3\n",
            "\n",
            "Elapsed time: 0:07:02.104935\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "import datetime \n",
        "from datetime import timedelta\n",
        "\n",
        "for i in range(1, 4): # Models 1, 2, 3\n",
        "\n",
        "  print(f\"Task {i+3}: Model {i}\\n\")\n",
        "  t = time.process_time() # to keep track of time\n",
        "\n",
        "  # Instantiate network\n",
        "  model = ConvNet(i)\n",
        "\n",
        "  # Initialize the criterion for loss computation \n",
        "  criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  # Initialize optimizer type \n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
        "\n",
        "  best_accuracy = 0.0\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(\"\\nEpoch: \", epoch)\n",
        "    train_loss, train_accuracy = train(model, trainloader, optimizer, criterion, epoch, batch_size)\n",
        "    test_loss, test_accuracy = test(model, testloader)\n",
        "    \n",
        "    if test_accuracy > best_accuracy:\n",
        "        best_accuracy = test_accuracy\n",
        "\n",
        "  print()       \n",
        "  print(\"Accuracy is {:2.2f}\".format(best_accuracy))\n",
        "\n",
        "  print(f\"Training and evaluation finished for Task {i+3}: Model {i}\\n\")\n",
        "\n",
        "  elapsed_time = time.process_time() - t\n",
        "  print(f\"Elapsed time: {str(datetime.timedelta(seconds=elapsed_time))}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss results ##\n",
        "\n",
        "|                        | Task 4      |Task 5      |Task 6    |\n",
        "| ---------------------  | ----------- |----------- |----------|\n",
        "| Training Accuracy      | 68%         |80%         |74%       |\n",
        "| Test Accuracy          | 62%         |69%         |70%       |\n",
        "| Training time in Colab | ~3 1/2 min  |~6 1/2 min  |~7 min    |\n",
        "\n",
        "<br>\n",
        "\n",
        "Starting from the simple CNN (task 4), task 5 increases the number of filters from 10 to 20 in the first Conv2d and from 10 to 40 in the second Conv2d improving the accuracy of the CNN. The number of filters is increased to increase the depth of the feature space (feature map) which helps in learning more levels of global abstract structures, and therefore, a more powerful model. \n",
        "\n",
        "Task 6 adds an additional convolutional layer for a total of 3 convolutional layers, but it does not have a max pooling operation after the third convolutional layer. As shown in the table above, it takes more time due to the additional convolution operation (element-wise matrix multiplication and addition) of the 3rd layer."
      ],
      "metadata": {
        "id": "TAIuAwG4v96l"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsYt/VdkWreSIHwI3LO+U3",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}